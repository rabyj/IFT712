# -*- coding: utf-8 -*-

#####
# Joanny Raby (15062245)
# Dona Chadid (20102835)
###

import numpy as np
import matplotlib.pyplot as plt


class MAPnoyau:
    def __init__(self, lamb=0.2, sigma_square=1.06, b=1.0, c=0.1, d=1.0, M=2, noyau='rbf'):
        """
        Classe effectuant de la segmentation de donnÃ©es 2D 2 classes Ã  l'aide de la mÃ©thode Ã  noyau.

        lamb: coefficiant de rÃ©gularisation L2
        sigma_square: paramÃ¨tre du noyau rbf
        b, d: paramÃ¨tres du noyau sigmoidal
        M,c: paramÃ¨tres du noyau polynomial
        noyau: rbf, lineaire, polynomial ou sigmoidal
        """
        self.lamb = lamb
        self.a = None
        self.sigma_square = sigma_square
        self.M = M
        self.c = c
        self.b = b
        self.d = d
        self.noyau = noyau
        self.x_train = None
    
    def entrainement(self, x_train, t_train):
        """
        EntraÃ®ne une mÃ©thode d'apprentissage Ã  noyau de type Maximum a
        posteriori (MAP) avec un terme d'attache aux donnÃ©es de type
        "moindre carrÃ©s" et un terme de lissage quadratique (voir
        Eq.(1.67) et Eq.(6.2) du livre de Bishop).  La variable x_train
        contient les entrÃ©es (un tableau 2D Numpy, oÃ¹ la n-iÃ¨me rangÃ©e
        correspond Ã  l'entrÃ©e x_n) et des cibles t_train (un tableau 1D Numpy
        oÃ¹ le n-iÃ¨me Ã©lÃ©ment correspond Ã  la cible t_n).

        L'entraÃ®nement doit utiliser un noyau de type RBF, lineaire, sigmoidal,
        ou polynomial (spÃ©cifiÃ© par ''self.noyau'') et dont les parametres
        sont contenus dans les variables self.sigma_square, self.c, self.b, self.d
        et self.M et un poids de rÃ©gularisation spÃ©cifiÃ© par ``self.lamb``.

        Cette mÃ©thode doit assigner le champs ``self.a`` tel que spÃ©cifiÃ© Ã 
        l'equation 6.8 du livre de Bishop et garder en mÃ©moire les donnÃ©es
        d'apprentissage dans ``self.x_train``
        """
        #AJOUTER CODE ICI
        
        I = np.identity(x_train.shape[0])
        
        # generate Gram matrix
        K = self.Gram(x_train, x_train)
                    
        # a=(K + Î»I)âˆ’1 t
        self.a = np.dot(np.linalg.inv(K + self.lamb * I), t_train)
        self.x_train = x_train
    
    def Gram(self, x, y):
        
        if self.noyau == "linear": # ğ‘˜(ğ‘¥,ğ‘¥â€²)=ğ‘¥.T ğ‘¥â€²
            K = x.dot(y.T)
            
        elif self.noyau == "polynomial": # ğ‘˜(ğ‘¥,ğ‘¥â€²)=(ğ‘¥.T ğ‘¥â€²+ğ‘)**ğ‘€
            K = (x.dot(y.T) + self.c) ** self.M
            
        elif self.noyau == "rbf": # ğ‘˜(ğ‘¥,ğ‘¥â€²)=expâ¡(âˆ’â€–ğ‘¥âˆ’ğ‘¥â€²â€–**2 / 2 * ğœ**2)
            K = np.zeros((x.shape[0], x.shape[0]))
            for i in range(x.shape[0]):
                for j in range(y.shape[0]):
                    K[i,j] = np.exp(-np.linalg.norm(x[i] - y[j])**2 / (2 * self.sigma_square))
        
        else: # ğ‘˜(ğ‘¥,ğ‘¥â€²)=tanh(ğ‘ğ‘¥ğ‘‡ğ‘¥â€²+ğ‘‘).
            K = np.tanh(self.b * x.dot(y.T) + self.d)
            
        return K
        
    def prediction(self, x):
        """
        Retourne la prÃ©diction pour une entrÃ©e representÃ©e par un tableau
        1D Numpy ``x``.

        Cette mÃ©thode suppose que la mÃ©thode ``entrainement()`` a prÃ©alablement
        Ã©tÃ© appelÃ©e. Elle doit utiliser le champs ``self.a`` afin de calculer
        la prÃ©diction y(x) (Ã©quation 6.9).

        NOTE : Puisque nous utilisons cette classe pour faire de la
        classification binaire, la prediction est +1 lorsque y(x)>0.5 et 0
        sinon
        """
        #AJOUTER CODE ICI
        
        k = self.Gram(x, self.x_train)
        predict = np.dot(k, self.a)
        return int(predict > 0.5)

    def erreur(self, t, prediction):
        """
        Retourne la diffÃ©rence au carrÃ© entre
        la cible ``t`` et la prÃ©diction ``prediction``.
        """
        # AJOUTER CODE ICI
        return (t-prediction)**2

    def validation_croisee(self, x_tab, t_tab):
        """
        Cette fonction trouve les meilleurs hyperparametres ``self.sigma_square``,
        ``self.c`` et ``self.M`` (tout dÃ©pendant du noyau selectionnÃ©) et
        ``self.lamb`` avec une validation croisÃ©e de type "k-fold" oÃ¹ k=10 avec les
        donnÃ©es contenues dans x_tab et t_tab.  Une fois les meilleurs hyperparamÃ¨tres
        trouvÃ©s, le modÃ¨le est entraÃ®nÃ© une derniÃ¨re fois.

        SUGGESTION: Les valeurs de ``self.sigma_square`` et ``self.lamb`` Ã  explorer vont
        de 0.000000001 Ã  2, les valeurs de ``self.c`` de 0 Ã  5, les valeurs
        de ''self.b'' et ''self.d'' de 0.00001 Ã  0.01 et ``self.M`` de 2 Ã  6
        """
        # AJOUTER CODE ICI

    def affichage(self, x_tab, t_tab):

        # Affichage
        ix = np.arange(x_tab[:, 0].min(), x_tab[:, 0].max(), 0.1)
        iy = np.arange(x_tab[:, 1].min(), x_tab[:, 1].max(), 0.1)
        iX, iY = np.meshgrid(ix, iy)
        x_vis = np.hstack([iX.reshape((-1, 1)), iY.reshape((-1, 1))])
        contour_out = np.array([self.prediction(x) for x in x_vis])
        contour_out = contour_out.reshape(iX.shape)

        plt.contourf(iX, iY, contour_out > 0.5)
        plt.scatter(x_tab[:, 0], x_tab[:, 1], s=(t_tab + 0.5) * 100, c=t_tab, edgecolors='y')
        plt.show()
