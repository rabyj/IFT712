# -*- coding: utf-8 -*-

#####
# Joanny Raby (15062245)
# Dona Chadid (20102835)
###

import numpy as np
import matplotlib.pyplot as plt


class MAPnoyau:
    def __init__(self, lamb=0.2, sigma_square=1.06, b=1.0, c=0.1, d=1.0, M=2, noyau='rbf'):
        """
        Classe effectuant de la segmentation de donn√©es 2D 2 classes √† l'aide de la m√©thode √† noyau.

        lamb: coefficiant de r√©gularisation L2
        sigma_square: param√®tre du noyau rbf
        b, d: param√®tres du noyau sigmoidal
        M,c: param√®tres du noyau polynomial
<<<<<<< HEAD
        noyau: rbf, lineaire, polynomial ou sigmoidal
=======
        noyau: rbf, lineaire, olynomial ou sigmoidal
>>>>>>> dev
        """
        self.lamb = lamb
        self.a = None
        self.sigma_square = sigma_square
        self.M = M
        self.c = c
        self.b = b
        self.d = d
        self.noyau = noyau
        self.x_train = None
<<<<<<< HEAD
    
=======

        

>>>>>>> dev
    def entrainement(self, x_train, t_train):
        """
        Entra√Æne une m√©thode d'apprentissage √† noyau de type Maximum a
        posteriori (MAP) avec un terme d'attache aux donn√©es de type
        "moindre carr√©s" et un terme de lissage quadratique (voir
        Eq.(1.67) et Eq.(6.2) du livre de Bishop).  La variable x_train
        contient les entr√©es (un tableau 2D Numpy, o√π la n-i√®me rang√©e
        correspond √† l'entr√©e x_n) et des cibles t_train (un tableau 1D Numpy
        o√π le n-i√®me √©l√©ment correspond √† la cible t_n).

        L'entra√Ænement doit utiliser un noyau de type RBF, lineaire, sigmoidal,
        ou polynomial (sp√©cifi√© par ''self.noyau'') et dont les parametres
        sont contenus dans les variables self.sigma_square, self.c, self.b, self.d
        et self.M et un poids de r√©gularisation sp√©cifi√© par ``self.lamb``.

        Cette m√©thode doit assigner le champs ``self.a`` tel que sp√©cifi√© √†
        l'equation 6.8 du livre de Bishop et garder en m√©moire les donn√©es
        d'apprentissage dans ``self.x_train``
        """
        #AJOUTER CODE ICI
        
<<<<<<< HEAD
        I = np.identity(x_train.shape[0])
        
        # generate Gram matrix
        K = self.Gram(x_train, x_train)
                    
        # a=(K + ŒªI)‚àí1 t
        self.a = np.dot(np.linalg.inv(K + self.lamb * I), t_train)
        self.x_train = x_train
    
    def Gram(self, x, y):
        
        if self.noyau == "linear": # ùëò(ùë•,ùë•‚Ä≤)=ùë•.T ùë•‚Ä≤
            K = x.dot(y.T)
            
        elif self.noyau == "polynomial": # ùëò(ùë•,ùë•‚Ä≤)=(ùë•.T ùë•‚Ä≤+ùëê)**ùëÄ
            K = (x.dot(y.T) + self.c) ** self.M
            
        elif self.noyau == "rbf": # ùëò(ùë•,ùë•‚Ä≤)=exp‚Å°(‚àí‚Äñùë•‚àíùë•‚Ä≤‚Äñ**2 / 2 * ùúé**2)
            K = np.zeros((x.shape[0], x.shape[0]))
            for i in range(x.shape[0]):
                for j in range(y.shape[0]):
                    K[i,j] = np.exp(-np.linalg.norm(x[i] - y[j])**2 / (2 * self.sigma_square))
        
        else: # ùëò(ùë•,ùë•‚Ä≤)=tanh(ùëèùë•ùëáùë•‚Ä≤+ùëë).
            K = np.tanh(self.b * x.dot(y.T) + self.d)
            
        return K
        
=======
>>>>>>> dev
    def prediction(self, x):
        """
        Retourne la pr√©diction pour une entr√©e represent√©e par un tableau
        1D Numpy ``x``.

        Cette m√©thode suppose que la m√©thode ``entrainement()`` a pr√©alablement
        √©t√© appel√©e. Elle doit utiliser le champs ``self.a`` afin de calculer
        la pr√©diction y(x) (√©quation 6.9).

        NOTE : Puisque nous utilisons cette classe pour faire de la
        classification binaire, la prediction est +1 lorsque y(x)>0.5 et 0
        sinon
        """
        #AJOUTER CODE ICI
<<<<<<< HEAD
        
        k = self.Gram(x, self.x_train)
        predict = np.dot(k, self.a)
        return int(predict > 0.5)
=======
        return 0
>>>>>>> dev

    def erreur(self, t, prediction):
        """
        Retourne la diff√©rence au carr√© entre
        la cible ``t`` et la pr√©diction ``prediction``.
        """
        # AJOUTER CODE ICI
<<<<<<< HEAD
        return (t-prediction)**2
=======
        return 0.
>>>>>>> dev

    def validation_croisee(self, x_tab, t_tab):
        """
        Cette fonction trouve les meilleurs hyperparametres ``self.sigma_square``,
        ``self.c`` et ``self.M`` (tout d√©pendant du noyau selectionn√©) et
        ``self.lamb`` avec une validation crois√©e de type "k-fold" o√π k=10 avec les
        donn√©es contenues dans x_tab et t_tab.  Une fois les meilleurs hyperparam√®tres
        trouv√©s, le mod√®le est entra√Æn√© une derni√®re fois.

        SUGGESTION: Les valeurs de ``self.sigma_square`` et ``self.lamb`` √† explorer vont
        de 0.000000001 √† 2, les valeurs de ``self.c`` de 0 √† 5, les valeurs
        de ''self.b'' et ''self.d'' de 0.00001 √† 0.01 et ``self.M`` de 2 √† 6
        """
        # AJOUTER CODE ICI

    def affichage(self, x_tab, t_tab):

        # Affichage
        ix = np.arange(x_tab[:, 0].min(), x_tab[:, 0].max(), 0.1)
        iy = np.arange(x_tab[:, 1].min(), x_tab[:, 1].max(), 0.1)
        iX, iY = np.meshgrid(ix, iy)
        x_vis = np.hstack([iX.reshape((-1, 1)), iY.reshape((-1, 1))])
        contour_out = np.array([self.prediction(x) for x in x_vis])
        contour_out = contour_out.reshape(iX.shape)

        plt.contourf(iX, iY, contour_out > 0.5)
        plt.scatter(x_tab[:, 0], x_tab[:, 1], s=(t_tab + 0.5) * 100, c=t_tab, edgecolors='y')
        plt.show()
